RAG Chatbot
===========

This repository contains a Retrieval-Augmented Generation (RAG) Chatbot implemented in Python using a Jupyter Notebook. The chatbot leverages the FLAN-T5 language model for text generation and ChromaDB for vector storage and retrieval, enabling users to upload documents (PDF or TXT) and ask questions based on their content. The application is built with a user-friendly Gradio interface, making it accessible via a web browser.

Features
--------

-   **Document Indexing**: Upload PDF or TXT files, which are chunked and indexed into a ChromaDB vector store for efficient retrieval.
-   **Context-Aware Responses**: Uses the FLAN-T5 model to generate answers based on the context retrieved from the indexed documents.
-   **Gradio Interface**: A simple web-based UI for uploading documents and querying the chatbot.
-   **Embedding Model**: Utilizes the `all-MiniLM-L6-v2` model from SentenceTransformers for generating document embeddings.

Requirements
------------

To run the chatbot, ensure you have the following dependencies installed. These are listed in the notebook and can be installed via pip:

```
pip install gradio PyPDF2 transformers sentence-transformers langchain-community chromadb

```

Additionally, the notebook is configured to run on a system with a GPU (e.g., Google Colab with a T4 GPU) for faster processing, though it can run on CPU as well.

Setup and Installation
----------------------

1.  **Clone the Repository**:

    ```
    git clone https://github.com/your-username/rag-chatbot.git
    cd rag-chatbot

    ```

2.  **Install Dependencies**:\
    Ensure you have Python 3.11 or later installed. Then, install the required packages:

    ```
    pip install -r requirements.txt

    ```

    Alternatively, you can manually install the packages listed above.

3.  **Run the Notebook**:\
    Open the `rag_chatbot.ipynb` notebook in Jupyter or Google Colab:

    ```
    jupyter notebook rag_chatbot.ipynb

    ```

    If using Google Colab, upload the notebook and ensure a GPU runtime is selected for optimal performance.

4.  **Launch the Gradio Interface**:\
    Execute the cells in the notebook sequentially. The final cell launches a Gradio app, which provides a public URL (if running on Colab) or a local URL for interaction.

Usage
-----

1.  **Upload a Document**:

    -   Use the Gradio interface to upload a PDF or TXT file.
    -   Click the "Process File" button to index the document. The text is extracted, chunked into smaller pieces (500 characters with 100-character overlap), and stored in a ChromaDB vector store.
2.  **Ask Questions**:

    -   Enter a question related to the uploaded document in the provided textbox.
    -   Click the "Get Answer" button to retrieve a response generated by the FLAN-T5 model, based on the most relevant document chunks retrieved via similarity search.
3.  **Example Workflow**:

    -   Upload a PDF containing a research paper.
    -   Ask a question like, "What is the main topic of the document?"
    -   The chatbot retrieves relevant chunks and generates a concise answer.

Code Structure
--------------

The notebook (`rag_chatbot.ipynb`) is organized into the following sections:

-   **Imports**: Loads necessary libraries, including Gradio, PyPDF2, Transformers, SentenceTransformers, LangChain, and ChromaDB.
-   **Loading Models**:
    -   Initializes the `all-MiniLM-L6-v2` model for embeddings.
    -   Loads the `google/flan-t5-base` model and tokenizer for text generation.
-   **Utility Functions**:
    -   `chunk_text`: Splits text into manageable chunks using LangChain's `RecursiveCharacterTextSplitter`.
    -   `extract_text`: Extracts text from PDF or TXT files.
    -   `process_file`: Indexes the document into ChromaDB.
-   **RAG Chat Function**:
    -   `chat`: Performs similarity search on the vector store, constructs a prompt with retrieved context, and generates a response using FLAN-T5.
-   **Gradio UI**: Defines the web interface for document upload and querying.

Notes
-----

-   **ChromaDB Persistence**: The notebook creates a persistent ChromaDB directory (`chroma_db`) to store document embeddings. This directory is cleared and recreated each time a new file is processed.
-   **Model Limitations**: The `google/flan-t5-base` model has a maximum input length of 512 tokens, and responses are limited to 128 new tokens. Adjust these parameters in the `chat` function if needed.
-   **Gradio Sharing**: When running on Colab, the Gradio app generates a public URL for sharing. This link expires after one week. For permanent hosting, consider deploying to Hugging Face Spaces using `gradio deploy`.
-   **Deprecation Warning**: The notebook includes a call to `db.persist()`, which may raise a deprecation warning as ChromaDB now handles persistence automatically. This does not affect functionality.

Example Output
--------------

Upon uploading a document and indexing it, you might see:

```
âœ… Successfully indexed 25 chunks from your document.

```

When asking a question:

```
User: What is the main topic of the document?
Chatbot: The document discusses the application of machine learning in natural language processing.

```

Acknowledgments
---------------

-   Built using the [Transformers](https://huggingface.co/transformers) library by Hugging Face.
-   Powered by [LangChain](https://github.com/langchain-ai/langchain) for document processing and vector storage.
-   Interface created with [Gradio](https://gradio.app/).
-   ChromaDB for efficient vector storage and retrieval.
